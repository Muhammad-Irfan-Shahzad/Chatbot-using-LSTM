{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import shutil\n",
        "\n",
        "# # Specify the path of the folder to be deleted\n",
        "# folder_path = '/content/input'\n",
        "\n",
        "# # Use shutil.rmtree to delete the folder and its contents\n",
        "# shutil.rmtree(folder_path)\n",
        "\n",
        "# print(f\"Folder '{folder_path}' and its contents deleted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHEyldIP5DhL",
        "outputId": "e33b81a1-effc-4f08-ce53-8b1c27633459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/input' and its contents deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/input.rar\"\n"
      ],
      "metadata": {
        "id": "hfXTRPxJgemT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e551907-a7fe-4c62-9fb6-cf8807103a8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/input.rar\n",
            "\n",
            "Creating    input                                                     OK\n",
            "Creating    input/chatbot                                             OK\n",
            "Extracting  input/chatbot/ai.yml                                         \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/botprofile.yml                                 \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/computers.yml                                  \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/emotion.yml                                    \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/food.yml                                       \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/gossip.yml                                     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/greetings.yml                                  \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/health.yml                                     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/movies.yml                                     \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  input/chatbot/psychology.yml                                 \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
        "import re\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:24.624591Z",
          "iopub.execute_input": "2022-04-29T10:56:24.625033Z",
          "iopub.status.idle": "2022-04-29T10:56:29.231477Z",
          "shell.execute_reply.started": "2022-04-29T10:56:24.624950Z",
          "shell.execute_reply": "2022-04-29T10:56:29.230765Z"
        },
        "trusted": true,
        "id": "BI-Pa-NFgWB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2ebed2-8f31-4d36-bd74-9e821baa345c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/input/chatbot/emotion.yml\n",
            "/content/input/chatbot/computers.yml\n",
            "/content/input/chatbot/health.yml\n",
            "/content/input/chatbot/greetings.yml\n",
            "/content/input/chatbot/botprofile.yml\n",
            "/content/input/chatbot/movies.yml\n",
            "/content/input/chatbot/gossip.yml\n",
            "/content/input/chatbot/food.yml\n",
            "/content/input/chatbot/psychology.yml\n",
            "/content/input/chatbot/ai.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data\n",
        "\n",
        "The dataset used is the Chatterbot dataset provided by Kaggle. The data is in the format of `yml` having question and answer pairs on various subjects like science, history, and psychology."
      ],
      "metadata": {
        "id": "2MDsw6wYgWB4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Q19Bc4ojiNg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "dir_path = '/content/input/chatbot'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:29.233160Z",
          "iopub.execute_input": "2022-04-29T10:56:29.233589Z",
          "iopub.status.idle": "2022-04-29T10:56:29.261001Z",
          "shell.execute_reply.started": "2022-04-29T10:56:29.233536Z",
          "shell.execute_reply": "2022-04-29T10:56:29.260345Z"
        },
        "trusted": true,
        "id": "DMeTKGcdgWB6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is split into question and answer lists. For our chatbot, we have used the `conversations` subject of the dataset."
      ],
      "metadata": {
        "id": "MpaB9cJbgWB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = [], []\n",
        "\n",
        "for filepath in files_list:\n",
        "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(file_)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len(con) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[1 :]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append(ans)\n",
        "        elif len(con)> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:29.262213Z",
          "iopub.execute_input": "2022-04-29T10:56:29.262620Z",
          "iopub.status.idle": "2022-04-29T10:56:29.632224Z",
          "shell.execute_reply.started": "2022-04-29T10:56:29.262582Z",
          "shell.execute_reply": "2022-04-29T10:56:29.631512Z"
        },
        "trusted": true,
        "id": "qJEINFM6gWB7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PufY7gOK1smX",
        "outputId": "b844828d-3129-40d2-de38-3b6d6ebdf04a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are arrogant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmkHlIKg1sou"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing for seq2seq learning\n",
        "\n",
        "- For preprocessing, a single vocabulary is used for tokenization.\n",
        "- The sequences are tokenized and padded. \\<start> and \\<end> tags are also appended to the sequences for the decoder input.\n",
        "- The above step is repeated for decoder output as well, except that the \\<start> tag is removed from all the sequences."
      ],
      "metadata": {
        "id": "EeCjjuPRgWB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers_with_tags = []\n",
        "for i in range(len(answers)):\n",
        "    if type(answers[i]) == str:\n",
        "        answers_with_tags.append(answers[i])\n",
        "    else:\n",
        "        questions.pop(i)\n",
        "\n",
        "answers = []\n",
        "for i in range(len(answers_with_tags)) :\n",
        "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:29.634062Z",
          "iopub.execute_input": "2022-04-29T10:56:29.634248Z",
          "iopub.status.idle": "2022-04-29T10:56:29.660179Z",
          "shell.execute_reply.started": "2022-04-29T10:56:29.634225Z",
          "shell.execute_reply": "2022-04-29T10:56:29.659587Z"
        },
        "trusted": true,
        "id": "TJSTDFzlgWB8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:29.661260Z",
          "iopub.execute_input": "2022-04-29T10:56:29.661567Z",
          "iopub.status.idle": "2022-04-29T10:56:29.671859Z",
          "shell.execute_reply.started": "2022-04-29T10:56:29.661520Z",
          "shell.execute_reply": "2022-04-29T10:56:29.671031Z"
        },
        "trusted": true,
        "id": "EJjQROocgWB8",
        "outputId": "9135dc72-9878-43fd-f308-fd6374b4a1eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1176"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append(tokens)\n",
        "    return tokens_list , vocabulary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:29.673061Z",
          "iopub.execute_input": "2022-04-29T10:56:29.673660Z",
          "iopub.status.idle": "2022-04-29T10:56:30.162425Z",
          "shell.execute_reply.started": "2022-04-29T10:56:29.673623Z",
          "shell.execute_reply": "2022-04-29T10:56:30.161740Z"
        },
        "trusted": true,
        "id": "Wxj-0bMlgWB-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCn7drOq9noM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
        "encoder_input_data = np.array(padded_questions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.163836Z",
          "iopub.execute_input": "2022-04-29T10:56:30.164081Z",
          "iopub.status.idle": "2022-04-29T10:56:30.177506Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.164047Z",
          "shell.execute_reply": "2022-04-29T10:56:30.176658Z"
        },
        "trusted": true,
        "id": "FgTbOQ23gWB-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_data.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.179969Z",
          "iopub.execute_input": "2022-04-29T10:56:30.180190Z",
          "iopub.status.idle": "2022-04-29T10:56:30.185252Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.180153Z",
          "shell.execute_reply": "2022-04-29T10:56:30.184484Z"
        },
        "trusted": true,
        "id": "JP1ppsBjgWB_",
        "outputId": "a12dc08c-d140-4fcf-f1a6-b0bbb9137052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(360, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
        "decoder_input_data = np.array(padded_answers)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.186845Z",
          "iopub.execute_input": "2022-04-29T10:56:30.187365Z",
          "iopub.status.idle": "2022-04-29T10:56:30.205242Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.187328Z",
          "shell.execute_reply": "2022-04-29T10:56:30.204526Z"
        },
        "trusted": true,
        "id": "g_Qmh0FYgWB_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_input_data.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.207940Z",
          "iopub.execute_input": "2022-04-29T10:56:30.208304Z",
          "iopub.status.idle": "2022-04-29T10:56:30.212222Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.208265Z",
          "shell.execute_reply": "2022-04-29T10:56:30.211468Z"
        },
        "trusted": true,
        "id": "v6zYPyTfgWCA",
        "outputId": "78b67808-2176-44c2-9b16-2f86e05119a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(360, 74)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
        "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_answers)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.213641Z",
          "iopub.execute_input": "2022-04-29T10:56:30.214232Z",
          "iopub.status.idle": "2022-04-29T10:56:30.462659Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.214090Z",
          "shell.execute_reply": "2022-04-29T10:56:30.461935Z"
        },
        "trusted": true,
        "id": "-hyflOUqgWCA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_output_data.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.464144Z",
          "iopub.execute_input": "2022-04-29T10:56:30.464603Z",
          "iopub.status.idle": "2022-04-29T10:56:30.469392Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.464567Z",
          "shell.execute_reply": "2022-04-29T10:56:30.468395Z"
        },
        "trusted": true,
        "id": "fEYhUkxFgWCB",
        "outputId": "70b13433-5112-4652-d9a4-404d13c7293f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(360, 74, 1176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model\n",
        "\n",
        "Keras Functional API is used to build the architecture of the model. The model is a multi input model, the encoder input and the decoder input. Successive layers include the Embedding and the LSTM layers"
      ],
      "metadata": {
        "id": "-sZ0ZJrogWCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding, LSTM and Desne layers\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
        "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax)\n",
        "output = decoder_dense (decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:30.470868Z",
          "iopub.execute_input": "2022-04-29T10:56:30.471121Z",
          "iopub.status.idle": "2022-04-29T10:56:33.907770Z",
          "shell.execute_reply.started": "2022-04-29T10:56:30.471087Z",
          "shell.execute_reply": "2022-04-29T10:56:33.907049Z"
        },
        "trusted": true,
        "id": "eb615xiGgWCC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:33.909008Z",
          "iopub.execute_input": "2022-04-29T10:56:33.909262Z",
          "iopub.status.idle": "2022-04-29T10:56:33.918982Z",
          "shell.execute_reply.started": "2022-04-29T10:56:33.909217Z",
          "shell.execute_reply": "2022-04-29T10:56:33.918300Z"
        },
        "trusted": true,
        "id": "tK7DGSRUgWCC",
        "outputId": "2b9e87f9-f07e-472b-d5d7-58fafaa7964b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 9)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 74)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 9, 200)               235200    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 74, 200)              235200    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 74, 200),            320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'lstm[0][1]',                \n",
            "                              (None, 200)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 74, 1176)             236376    ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1348376 (5.14 MB)\n",
            "Trainable params: 1348376 (5.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=32, epochs=350)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:56:33.920233Z",
          "iopub.execute_input": "2022-04-29T10:56:33.920985Z",
          "iopub.status.idle": "2022-04-29T10:58:04.358994Z",
          "shell.execute_reply.started": "2022-04-29T10:56:33.920933Z",
          "shell.execute_reply": "2022-04-29T10:58:04.358339Z"
        },
        "trusted": true,
        "id": "1r4bYuycgWCD",
        "outputId": "fbf764fb-d51d-410e-966b-b9e7b558b368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/350\n",
            "12/12 [==============================] - 4s 300ms/step - loss: 0.9534\n",
            "Epoch 2/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.9437\n",
            "Epoch 3/350\n",
            "12/12 [==============================] - 4s 346ms/step - loss: 0.9293\n",
            "Epoch 4/350\n",
            "12/12 [==============================] - 4s 344ms/step - loss: 0.9221\n",
            "Epoch 5/350\n",
            "12/12 [==============================] - 4s 339ms/step - loss: 0.9059\n",
            "Epoch 6/350\n",
            "12/12 [==============================] - 5s 386ms/step - loss: 0.9061\n",
            "Epoch 7/350\n",
            "12/12 [==============================] - 4s 331ms/step - loss: 0.8978\n",
            "Epoch 8/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.8872\n",
            "Epoch 9/350\n",
            "12/12 [==============================] - 3s 290ms/step - loss: 0.8595\n",
            "Epoch 10/350\n",
            "12/12 [==============================] - 5s 387ms/step - loss: 0.8662\n",
            "Epoch 11/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.8488\n",
            "Epoch 12/350\n",
            "12/12 [==============================] - 3s 289ms/step - loss: 0.8420\n",
            "Epoch 13/350\n",
            "12/12 [==============================] - 5s 404ms/step - loss: 0.8350\n",
            "Epoch 14/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.8261\n",
            "Epoch 15/350\n",
            "12/12 [==============================] - 4s 288ms/step - loss: 0.8191\n",
            "Epoch 16/350\n",
            "12/12 [==============================] - 5s 393ms/step - loss: 0.8017\n",
            "Epoch 17/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.8021\n",
            "Epoch 18/350\n",
            "12/12 [==============================] - 4s 304ms/step - loss: 0.7887\n",
            "Epoch 19/350\n",
            "12/12 [==============================] - 4s 343ms/step - loss: 0.7829\n",
            "Epoch 20/350\n",
            "12/12 [==============================] - 4s 317ms/step - loss: 0.7697\n",
            "Epoch 21/350\n",
            "12/12 [==============================] - 3s 285ms/step - loss: 0.7571\n",
            "Epoch 22/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.7600\n",
            "Epoch 23/350\n",
            "12/12 [==============================] - 5s 386ms/step - loss: 0.7423\n",
            "Epoch 24/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.7367\n",
            "Epoch 25/350\n",
            "12/12 [==============================] - 4s 306ms/step - loss: 0.7291\n",
            "Epoch 26/350\n",
            "12/12 [==============================] - 5s 415ms/step - loss: 0.7176\n",
            "Epoch 27/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.7121\n",
            "Epoch 28/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.6936\n",
            "Epoch 29/350\n",
            "12/12 [==============================] - 5s 385ms/step - loss: 0.6931\n",
            "Epoch 30/350\n",
            "12/12 [==============================] - 4s 289ms/step - loss: 0.6881\n",
            "Epoch 31/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.6735\n",
            "Epoch 32/350\n",
            "12/12 [==============================] - 4s 328ms/step - loss: 0.6713\n",
            "Epoch 33/350\n",
            "12/12 [==============================] - 4s 343ms/step - loss: 0.6631\n",
            "Epoch 34/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.6548\n",
            "Epoch 35/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.6486\n",
            "Epoch 36/350\n",
            "12/12 [==============================] - 5s 380ms/step - loss: 0.6431\n",
            "Epoch 37/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.6327\n",
            "Epoch 38/350\n",
            "12/12 [==============================] - 3s 284ms/step - loss: 0.6228\n",
            "Epoch 39/350\n",
            "12/12 [==============================] - 5s 401ms/step - loss: 0.6212\n",
            "Epoch 40/350\n",
            "12/12 [==============================] - 4s 311ms/step - loss: 0.6110\n",
            "Epoch 41/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.6016\n",
            "Epoch 42/350\n",
            "12/12 [==============================] - 4s 371ms/step - loss: 0.5966\n",
            "Epoch 43/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.5870\n",
            "Epoch 44/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.5804\n",
            "Epoch 45/350\n",
            "12/12 [==============================] - 4s 320ms/step - loss: 0.5790\n",
            "Epoch 46/350\n",
            "12/12 [==============================] - 4s 335ms/step - loss: 0.5674\n",
            "Epoch 47/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.5586\n",
            "Epoch 48/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.5467\n",
            "Epoch 49/350\n",
            "12/12 [==============================] - 5s 397ms/step - loss: 0.5574\n",
            "Epoch 50/350\n",
            "12/12 [==============================] - 3s 284ms/step - loss: 0.5379\n",
            "Epoch 51/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.5386\n",
            "Epoch 52/350\n",
            "12/12 [==============================] - 5s 389ms/step - loss: 0.5288\n",
            "Epoch 53/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.5243\n",
            "Epoch 54/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.5174\n",
            "Epoch 55/350\n",
            "12/12 [==============================] - 4s 345ms/step - loss: 0.5443\n",
            "Epoch 56/350\n",
            "12/12 [==============================] - 4s 326ms/step - loss: 0.4965\n",
            "Epoch 57/350\n",
            "12/12 [==============================] - 4s 304ms/step - loss: 0.4946\n",
            "Epoch 58/350\n",
            "12/12 [==============================] - 4s 320ms/step - loss: 0.4868\n",
            "Epoch 59/350\n",
            "12/12 [==============================] - 4s 357ms/step - loss: 0.4861\n",
            "Epoch 60/350\n",
            "12/12 [==============================] - 3s 289ms/step - loss: 0.4807\n",
            "Epoch 61/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.4712\n",
            "Epoch 62/350\n",
            "12/12 [==============================] - 5s 400ms/step - loss: 0.4664\n",
            "Epoch 63/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.4695\n",
            "Epoch 64/350\n",
            "12/12 [==============================] - 4s 313ms/step - loss: 0.4620\n",
            "Epoch 65/350\n",
            "12/12 [==============================] - 5s 409ms/step - loss: 0.4532\n",
            "Epoch 66/350\n",
            "12/12 [==============================] - 4s 311ms/step - loss: 0.4466\n",
            "Epoch 67/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.4452\n",
            "Epoch 68/350\n",
            "12/12 [==============================] - 5s 414ms/step - loss: 0.4337\n",
            "Epoch 69/350\n",
            "12/12 [==============================] - 5s 378ms/step - loss: 0.4374\n",
            "Epoch 70/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.4294\n",
            "Epoch 71/350\n",
            "12/12 [==============================] - 4s 347ms/step - loss: 0.4228\n",
            "Epoch 72/350\n",
            "12/12 [==============================] - 4s 343ms/step - loss: 0.4143\n",
            "Epoch 73/350\n",
            "12/12 [==============================] - 3s 289ms/step - loss: 0.4095\n",
            "Epoch 74/350\n",
            "12/12 [==============================] - 3s 284ms/step - loss: 0.4071\n",
            "Epoch 75/350\n",
            "12/12 [==============================] - 5s 393ms/step - loss: 0.4008\n",
            "Epoch 76/350\n",
            "12/12 [==============================] - 3s 283ms/step - loss: 0.3980\n",
            "Epoch 77/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.3933\n",
            "Epoch 78/350\n",
            "12/12 [==============================] - 5s 404ms/step - loss: 0.3812\n",
            "Epoch 79/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.3831\n",
            "Epoch 80/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.3778\n",
            "Epoch 81/350\n",
            "12/12 [==============================] - 5s 384ms/step - loss: 0.3755\n",
            "Epoch 82/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.3726\n",
            "Epoch 83/350\n",
            "12/12 [==============================] - 4s 294ms/step - loss: 0.3659\n",
            "Epoch 84/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.3635\n",
            "Epoch 85/350\n",
            "12/12 [==============================] - 4s 347ms/step - loss: 0.3687\n",
            "Epoch 86/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.3515\n",
            "Epoch 87/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.3466\n",
            "Epoch 88/350\n",
            "12/12 [==============================] - 5s 395ms/step - loss: 0.3448\n",
            "Epoch 89/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.3392\n",
            "Epoch 90/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.3385\n",
            "Epoch 91/350\n",
            "12/12 [==============================] - 5s 390ms/step - loss: 0.3364\n",
            "Epoch 92/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.3285\n",
            "Epoch 93/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.3251\n",
            "Epoch 94/350\n",
            "12/12 [==============================] - 4s 378ms/step - loss: 0.3206\n",
            "Epoch 95/350\n",
            "12/12 [==============================] - 4s 306ms/step - loss: 0.3160\n",
            "Epoch 96/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.3144\n",
            "Epoch 97/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.3097\n",
            "Epoch 98/350\n",
            "12/12 [==============================] - 4s 358ms/step - loss: 0.3134\n",
            "Epoch 99/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.3010\n",
            "Epoch 100/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.2994\n",
            "Epoch 101/350\n",
            "12/12 [==============================] - 5s 385ms/step - loss: 0.2999\n",
            "Epoch 102/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.2943\n",
            "Epoch 103/350\n",
            "12/12 [==============================] - 3s 288ms/step - loss: 0.2886\n",
            "Epoch 104/350\n",
            "12/12 [==============================] - 5s 404ms/step - loss: 0.2844\n",
            "Epoch 105/350\n",
            "12/12 [==============================] - 4s 288ms/step - loss: 0.2823\n",
            "Epoch 106/350\n",
            "12/12 [==============================] - 3s 283ms/step - loss: 0.2840\n",
            "Epoch 107/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.2754\n",
            "Epoch 108/350\n",
            "12/12 [==============================] - 4s 355ms/step - loss: 0.2713\n",
            "Epoch 109/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.2670\n",
            "Epoch 110/350\n",
            "12/12 [==============================] - 4s 314ms/step - loss: 0.2688\n",
            "Epoch 111/350\n",
            "12/12 [==============================] - 5s 398ms/step - loss: 0.2638\n",
            "Epoch 112/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.2594\n",
            "Epoch 113/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.2630\n",
            "Epoch 114/350\n",
            "12/12 [==============================] - 5s 390ms/step - loss: 0.2553\n",
            "Epoch 115/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.2544\n",
            "Epoch 116/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.2509\n",
            "Epoch 117/350\n",
            "12/12 [==============================] - 4s 375ms/step - loss: 0.2465\n",
            "Epoch 118/350\n",
            "12/12 [==============================] - 4s 300ms/step - loss: 0.2432\n",
            "Epoch 119/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.2460\n",
            "Epoch 120/350\n",
            "12/12 [==============================] - 4s 308ms/step - loss: 0.2437\n",
            "Epoch 121/350\n",
            "12/12 [==============================] - 4s 352ms/step - loss: 0.2357\n",
            "Epoch 122/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.2332\n",
            "Epoch 123/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.2347\n",
            "Epoch 124/350\n",
            "12/12 [==============================] - 5s 400ms/step - loss: 0.2301\n",
            "Epoch 125/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.2264\n",
            "Epoch 126/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.2292\n",
            "Epoch 127/350\n",
            "12/12 [==============================] - 5s 395ms/step - loss: 0.2228\n",
            "Epoch 128/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.2184\n",
            "Epoch 129/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.2175\n",
            "Epoch 130/350\n",
            "12/12 [==============================] - 4s 338ms/step - loss: 0.2129\n",
            "Epoch 131/350\n",
            "12/12 [==============================] - 4s 326ms/step - loss: 0.2141\n",
            "Epoch 132/350\n",
            "12/12 [==============================] - 4s 362ms/step - loss: 0.2126\n",
            "Epoch 133/350\n",
            "12/12 [==============================] - 5s 373ms/step - loss: 0.2125\n",
            "Epoch 134/350\n",
            "12/12 [==============================] - 4s 313ms/step - loss: 0.2035\n",
            "Epoch 135/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.2045\n",
            "Epoch 136/350\n",
            "12/12 [==============================] - 3s 284ms/step - loss: 0.2047\n",
            "Epoch 137/350\n",
            "12/12 [==============================] - 5s 393ms/step - loss: 0.2001\n",
            "Epoch 138/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.1977\n",
            "Epoch 139/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1952\n",
            "Epoch 140/350\n",
            "12/12 [==============================] - 5s 404ms/step - loss: 0.2000\n",
            "Epoch 141/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1963\n",
            "Epoch 142/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1906\n",
            "Epoch 143/350\n",
            "12/12 [==============================] - 5s 386ms/step - loss: 0.1893\n",
            "Epoch 144/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1873\n",
            "Epoch 145/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1867\n",
            "Epoch 146/350\n",
            "12/12 [==============================] - 4s 336ms/step - loss: 0.1820\n",
            "Epoch 147/350\n",
            "12/12 [==============================] - 4s 333ms/step - loss: 0.1804\n",
            "Epoch 148/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.1824\n",
            "Epoch 149/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1778\n",
            "Epoch 150/350\n",
            "12/12 [==============================] - 5s 394ms/step - loss: 0.1778\n",
            "Epoch 151/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.1761\n",
            "Epoch 152/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.1731\n",
            "Epoch 153/350\n",
            "12/12 [==============================] - 5s 425ms/step - loss: 0.1721\n",
            "Epoch 154/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1677\n",
            "Epoch 155/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.1760\n",
            "Epoch 156/350\n",
            "12/12 [==============================] - 5s 407ms/step - loss: 0.1681\n",
            "Epoch 157/350\n",
            "12/12 [==============================] - 4s 313ms/step - loss: 0.1663\n",
            "Epoch 158/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.1636\n",
            "Epoch 159/350\n",
            "12/12 [==============================] - 4s 378ms/step - loss: 0.1616\n",
            "Epoch 160/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.1610\n",
            "Epoch 161/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.1611\n",
            "Epoch 162/350\n",
            "12/12 [==============================] - 4s 326ms/step - loss: 0.1597\n",
            "Epoch 163/350\n",
            "12/12 [==============================] - 4s 357ms/step - loss: 0.1547\n",
            "Epoch 164/350\n",
            "12/12 [==============================] - 3s 287ms/step - loss: 0.1559\n",
            "Epoch 165/350\n",
            "12/12 [==============================] - 4s 300ms/step - loss: 0.1526\n",
            "Epoch 166/350\n",
            "12/12 [==============================] - 5s 390ms/step - loss: 0.1532\n",
            "Epoch 167/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.1520\n",
            "Epoch 168/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.1498\n",
            "Epoch 169/350\n",
            "12/12 [==============================] - 5s 405ms/step - loss: 0.1478\n",
            "Epoch 170/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.1491\n",
            "Epoch 171/350\n",
            "12/12 [==============================] - 4s 306ms/step - loss: 0.1459\n",
            "Epoch 172/350\n",
            "12/12 [==============================] - 5s 393ms/step - loss: 0.1428\n",
            "Epoch 173/350\n",
            "12/12 [==============================] - 4s 287ms/step - loss: 0.1457\n",
            "Epoch 174/350\n",
            "12/12 [==============================] - 3s 289ms/step - loss: 0.1435\n",
            "Epoch 175/350\n",
            "12/12 [==============================] - 4s 333ms/step - loss: 0.1431\n",
            "Epoch 176/350\n",
            "12/12 [==============================] - 4s 356ms/step - loss: 0.1415\n",
            "Epoch 177/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1413\n",
            "Epoch 178/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1376\n",
            "Epoch 179/350\n",
            "12/12 [==============================] - 5s 392ms/step - loss: 0.1387\n",
            "Epoch 180/350\n",
            "12/12 [==============================] - 4s 294ms/step - loss: 0.1369\n",
            "Epoch 181/350\n",
            "12/12 [==============================] - 3s 287ms/step - loss: 0.1367\n",
            "Epoch 182/350\n",
            "12/12 [==============================] - 5s 393ms/step - loss: 0.1373\n",
            "Epoch 183/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.1340\n",
            "Epoch 184/350\n",
            "12/12 [==============================] - 3s 290ms/step - loss: 0.1323\n",
            "Epoch 185/350\n",
            "12/12 [==============================] - 5s 385ms/step - loss: 0.1306\n",
            "Epoch 186/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.1303\n",
            "Epoch 187/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.1287\n",
            "Epoch 188/350\n",
            "12/12 [==============================] - 4s 334ms/step - loss: 0.1266\n",
            "Epoch 189/350\n",
            "12/12 [==============================] - 4s 339ms/step - loss: 0.1281\n",
            "Epoch 190/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.1277\n",
            "Epoch 191/350\n",
            "12/12 [==============================] - 3s 283ms/step - loss: 0.1245\n",
            "Epoch 192/350\n",
            "12/12 [==============================] - 5s 389ms/step - loss: 0.1233\n",
            "Epoch 193/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1256\n",
            "Epoch 194/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.1228\n",
            "Epoch 195/350\n",
            "12/12 [==============================] - 5s 458ms/step - loss: 0.1213\n",
            "Epoch 196/350\n",
            "12/12 [==============================] - 4s 316ms/step - loss: 0.1218\n",
            "Epoch 197/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.1203\n",
            "Epoch 198/350\n",
            "12/12 [==============================] - 4s 309ms/step - loss: 0.1188\n",
            "Epoch 199/350\n",
            "12/12 [==============================] - 4s 355ms/step - loss: 0.1168\n",
            "Epoch 200/350\n",
            "12/12 [==============================] - 4s 308ms/step - loss: 0.1193\n",
            "Epoch 201/350\n",
            "12/12 [==============================] - 4s 296ms/step - loss: 0.1184\n",
            "Epoch 202/350\n",
            "12/12 [==============================] - 5s 398ms/step - loss: 0.1166\n",
            "Epoch 203/350\n",
            "12/12 [==============================] - 4s 300ms/step - loss: 0.1147\n",
            "Epoch 204/350\n",
            "12/12 [==============================] - 3s 288ms/step - loss: 0.1170\n",
            "Epoch 205/350\n",
            "12/12 [==============================] - 5s 391ms/step - loss: 0.1123\n",
            "Epoch 206/350\n",
            "12/12 [==============================] - 3s 283ms/step - loss: 0.1140\n",
            "Epoch 207/350\n",
            "12/12 [==============================] - 3s 284ms/step - loss: 0.1117\n",
            "Epoch 208/350\n",
            "12/12 [==============================] - 4s 358ms/step - loss: 0.1113\n",
            "Epoch 209/350\n",
            "12/12 [==============================] - 4s 317ms/step - loss: 0.1112\n",
            "Epoch 210/350\n",
            "12/12 [==============================] - 3s 285ms/step - loss: 0.1095\n",
            "Epoch 211/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.1121\n",
            "Epoch 212/350\n",
            "12/12 [==============================] - 5s 381ms/step - loss: 0.1084\n",
            "Epoch 213/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.1092\n",
            "Epoch 214/350\n",
            "12/12 [==============================] - 4s 304ms/step - loss: 0.1084\n",
            "Epoch 215/350\n",
            "12/12 [==============================] - 5s 390ms/step - loss: 0.1059\n",
            "Epoch 216/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.1040\n",
            "Epoch 217/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.1055\n",
            "Epoch 218/350\n",
            "12/12 [==============================] - 5s 400ms/step - loss: 0.1057\n",
            "Epoch 219/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.1033\n",
            "Epoch 220/350\n",
            "12/12 [==============================] - 3s 285ms/step - loss: 0.1031\n",
            "Epoch 221/350\n",
            "12/12 [==============================] - 4s 347ms/step - loss: 0.1031\n",
            "Epoch 222/350\n",
            "12/12 [==============================] - 4s 334ms/step - loss: 0.1028\n",
            "Epoch 223/350\n",
            "12/12 [==============================] - 3s 286ms/step - loss: 0.1034\n",
            "Epoch 224/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.1010\n",
            "Epoch 225/350\n",
            "12/12 [==============================] - 5s 388ms/step - loss: 0.1006\n",
            "Epoch 226/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.0992\n",
            "Epoch 227/350\n",
            "12/12 [==============================] - 3s 288ms/step - loss: 0.0996\n",
            "Epoch 228/350\n",
            "12/12 [==============================] - 5s 397ms/step - loss: 0.1002\n",
            "Epoch 229/350\n",
            "12/12 [==============================] - 3s 281ms/step - loss: 0.0989\n",
            "Epoch 230/350\n",
            "12/12 [==============================] - 3s 287ms/step - loss: 0.0972\n",
            "Epoch 231/350\n",
            "12/12 [==============================] - 5s 401ms/step - loss: 0.0963\n",
            "Epoch 232/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0964\n",
            "Epoch 233/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.0961\n",
            "Epoch 234/350\n",
            "12/12 [==============================] - 4s 371ms/step - loss: 0.0948\n",
            "Epoch 235/350\n",
            "12/12 [==============================] - 4s 319ms/step - loss: 0.0955\n",
            "Epoch 236/350\n",
            "12/12 [==============================] - 4s 295ms/step - loss: 0.0945\n",
            "Epoch 237/350\n",
            "12/12 [==============================] - 4s 297ms/step - loss: 0.0934\n",
            "Epoch 238/350\n",
            "12/12 [==============================] - 5s 385ms/step - loss: 0.0940\n",
            "Epoch 239/350\n",
            "12/12 [==============================] - 4s 308ms/step - loss: 0.0937\n",
            "Epoch 240/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0935\n",
            "Epoch 241/350\n",
            "12/12 [==============================] - 5s 398ms/step - loss: 0.0907\n",
            "Epoch 242/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.0915\n",
            "Epoch 243/350\n",
            "12/12 [==============================] - 4s 308ms/step - loss: 0.0947\n",
            "Epoch 244/350\n",
            "12/12 [==============================] - 5s 411ms/step - loss: 0.0908\n",
            "Epoch 245/350\n",
            "12/12 [==============================] - 4s 306ms/step - loss: 0.0900\n",
            "Epoch 246/350\n",
            "12/12 [==============================] - 4s 321ms/step - loss: 0.0906\n",
            "Epoch 247/350\n",
            "12/12 [==============================] - 5s 414ms/step - loss: 0.0885\n",
            "Epoch 248/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.0890\n",
            "Epoch 249/350\n",
            "12/12 [==============================] - 4s 300ms/step - loss: 0.0874\n",
            "Epoch 250/350\n",
            "12/12 [==============================] - 5s 392ms/step - loss: 0.0878\n",
            "Epoch 251/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.0885\n",
            "Epoch 252/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.0873\n",
            "Epoch 253/350\n",
            "12/12 [==============================] - 4s 348ms/step - loss: 0.0861\n",
            "Epoch 254/350\n",
            "12/12 [==============================] - 5s 360ms/step - loss: 0.0863\n",
            "Epoch 255/350\n",
            "12/12 [==============================] - 4s 291ms/step - loss: 0.0863\n",
            "Epoch 256/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.0860\n",
            "Epoch 257/350\n",
            "12/12 [==============================] - 5s 452ms/step - loss: 0.0911\n",
            "Epoch 258/350\n",
            "12/12 [==============================] - 4s 341ms/step - loss: 0.0863\n",
            "Epoch 259/350\n",
            "12/12 [==============================] - 4s 340ms/step - loss: 0.0839\n",
            "Epoch 260/350\n",
            "12/12 [==============================] - 4s 360ms/step - loss: 0.0836\n",
            "Epoch 261/350\n",
            "12/12 [==============================] - 4s 318ms/step - loss: 0.0836\n",
            "Epoch 262/350\n",
            "12/12 [==============================] - 4s 331ms/step - loss: 0.0829\n",
            "Epoch 263/350\n",
            "12/12 [==============================] - 5s 389ms/step - loss: 0.0827\n",
            "Epoch 264/350\n",
            "12/12 [==============================] - 4s 306ms/step - loss: 0.0832\n",
            "Epoch 265/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.0834\n",
            "Epoch 266/350\n",
            "12/12 [==============================] - 5s 400ms/step - loss: 0.0830\n",
            "Epoch 267/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.0826\n",
            "Epoch 268/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.0810\n",
            "Epoch 269/350\n",
            "12/12 [==============================] - 5s 408ms/step - loss: 0.0808\n",
            "Epoch 270/350\n",
            "12/12 [==============================] - 4s 309ms/step - loss: 0.0814\n",
            "Epoch 271/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0790\n",
            "Epoch 272/350\n",
            "12/12 [==============================] - 5s 406ms/step - loss: 0.0783\n",
            "Epoch 273/350\n",
            "12/12 [==============================] - 4s 294ms/step - loss: 0.0792\n",
            "Epoch 274/350\n",
            "12/12 [==============================] - 4s 316ms/step - loss: 0.0791\n",
            "Epoch 275/350\n",
            "12/12 [==============================] - 5s 406ms/step - loss: 0.0791\n",
            "Epoch 276/350\n",
            "12/12 [==============================] - 4s 318ms/step - loss: 0.0786\n",
            "Epoch 277/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.0775\n",
            "Epoch 278/350\n",
            "12/12 [==============================] - 4s 379ms/step - loss: 0.0772\n",
            "Epoch 279/350\n",
            "12/12 [==============================] - 4s 325ms/step - loss: 0.0772\n",
            "Epoch 280/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0815\n",
            "Epoch 281/350\n",
            "12/12 [==============================] - 4s 353ms/step - loss: 0.0783\n",
            "Epoch 282/350\n",
            "12/12 [==============================] - 4s 357ms/step - loss: 0.0770\n",
            "Epoch 283/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.0770\n",
            "Epoch 284/350\n",
            "12/12 [==============================] - 4s 313ms/step - loss: 0.0745\n",
            "Epoch 285/350\n",
            "12/12 [==============================] - 5s 388ms/step - loss: 0.0769\n",
            "Epoch 286/350\n",
            "12/12 [==============================] - 4s 298ms/step - loss: 0.0791\n",
            "Epoch 287/350\n",
            "12/12 [==============================] - 4s 309ms/step - loss: 0.0762\n",
            "Epoch 288/350\n",
            "12/12 [==============================] - 5s 419ms/step - loss: 0.0740\n",
            "Epoch 289/350\n",
            "12/12 [==============================] - 4s 304ms/step - loss: 0.0743\n",
            "Epoch 290/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.0753\n",
            "Epoch 291/350\n",
            "12/12 [==============================] - 5s 421ms/step - loss: 0.0740\n",
            "Epoch 292/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.0764\n",
            "Epoch 293/350\n",
            "12/12 [==============================] - 4s 311ms/step - loss: 0.0743\n",
            "Epoch 294/350\n",
            "12/12 [==============================] - 5s 405ms/step - loss: 0.0726\n",
            "Epoch 295/350\n",
            "12/12 [==============================] - 4s 315ms/step - loss: 0.0726\n",
            "Epoch 296/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.0731\n",
            "Epoch 297/350\n",
            "12/12 [==============================] - 5s 400ms/step - loss: 0.0751\n",
            "Epoch 298/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.0730\n",
            "Epoch 299/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.0715\n",
            "Epoch 300/350\n",
            "12/12 [==============================] - 4s 373ms/step - loss: 0.0718\n",
            "Epoch 301/350\n",
            "12/12 [==============================] - 4s 327ms/step - loss: 0.0718\n",
            "Epoch 302/350\n",
            "12/12 [==============================] - 4s 308ms/step - loss: 0.0700\n",
            "Epoch 303/350\n",
            "12/12 [==============================] - 4s 341ms/step - loss: 0.0718\n",
            "Epoch 304/350\n",
            "12/12 [==============================] - 5s 362ms/step - loss: 0.0702\n",
            "Epoch 305/350\n",
            "12/12 [==============================] - 4s 316ms/step - loss: 0.0715\n",
            "Epoch 306/350\n",
            "12/12 [==============================] - 4s 339ms/step - loss: 0.0707\n",
            "Epoch 307/350\n",
            "12/12 [==============================] - 5s 391ms/step - loss: 0.0704\n",
            "Epoch 308/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.0700\n",
            "Epoch 309/350\n",
            "12/12 [==============================] - 4s 304ms/step - loss: 0.0697\n",
            "Epoch 310/350\n",
            "12/12 [==============================] - 5s 405ms/step - loss: 0.0688\n",
            "Epoch 311/350\n",
            "12/12 [==============================] - 4s 315ms/step - loss: 0.0687\n",
            "Epoch 312/350\n",
            "12/12 [==============================] - 4s 316ms/step - loss: 0.0696\n",
            "Epoch 313/350\n",
            "12/12 [==============================] - 5s 413ms/step - loss: 0.0688\n",
            "Epoch 314/350\n",
            "12/12 [==============================] - 4s 301ms/step - loss: 0.0684\n",
            "Epoch 315/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0679\n",
            "Epoch 316/350\n",
            "12/12 [==============================] - 5s 407ms/step - loss: 0.0687\n",
            "Epoch 317/350\n",
            "12/12 [==============================] - 4s 328ms/step - loss: 0.0681\n",
            "Epoch 318/350\n",
            "12/12 [==============================] - 5s 366ms/step - loss: 0.0682\n",
            "Epoch 319/350\n",
            "12/12 [==============================] - 5s 406ms/step - loss: 0.0689\n",
            "Epoch 320/350\n",
            "12/12 [==============================] - 4s 310ms/step - loss: 0.0677\n",
            "Epoch 321/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.0664\n",
            "Epoch 322/350\n",
            "12/12 [==============================] - 5s 398ms/step - loss: 0.0662\n",
            "Epoch 323/350\n",
            "12/12 [==============================] - 4s 313ms/step - loss: 0.0674\n",
            "Epoch 324/350\n",
            "12/12 [==============================] - 4s 302ms/step - loss: 0.0674\n",
            "Epoch 325/350\n",
            "12/12 [==============================] - 5s 413ms/step - loss: 0.0673\n",
            "Epoch 326/350\n",
            "12/12 [==============================] - 4s 317ms/step - loss: 0.0658\n",
            "Epoch 327/350\n",
            "12/12 [==============================] - 4s 314ms/step - loss: 0.0665\n",
            "Epoch 328/350\n",
            "12/12 [==============================] - 5s 421ms/step - loss: 0.0660\n",
            "Epoch 329/350\n",
            "12/12 [==============================] - 4s 305ms/step - loss: 0.0679\n",
            "Epoch 330/350\n",
            "12/12 [==============================] - 4s 292ms/step - loss: 0.0664\n",
            "Epoch 331/350\n",
            "12/12 [==============================] - 4s 368ms/step - loss: 0.0659\n",
            "Epoch 332/350\n",
            "12/12 [==============================] - 4s 315ms/step - loss: 0.0646\n",
            "Epoch 333/350\n",
            "12/12 [==============================] - 4s 307ms/step - loss: 0.0662\n",
            "Epoch 334/350\n",
            "12/12 [==============================] - 5s 417ms/step - loss: 0.0662\n",
            "Epoch 335/350\n",
            "12/12 [==============================] - 4s 352ms/step - loss: 0.0667\n",
            "Epoch 336/350\n",
            "12/12 [==============================] - 4s 312ms/step - loss: 0.0633\n",
            "Epoch 337/350\n",
            "12/12 [==============================] - 4s 332ms/step - loss: 0.0648\n",
            "Epoch 338/350\n",
            "12/12 [==============================] - 5s 362ms/step - loss: 0.0628\n",
            "Epoch 339/350\n",
            "12/12 [==============================] - 4s 294ms/step - loss: 0.0638\n",
            "Epoch 340/350\n",
            "12/12 [==============================] - 4s 303ms/step - loss: 0.0634\n",
            "Epoch 341/350\n",
            "12/12 [==============================] - 5s 405ms/step - loss: 0.0640\n",
            "Epoch 342/350\n",
            "12/12 [==============================] - 4s 290ms/step - loss: 0.0709\n",
            "Epoch 343/350\n",
            "12/12 [==============================] - 3s 281ms/step - loss: 0.0642\n",
            "Epoch 344/350\n",
            "12/12 [==============================] - 5s 418ms/step - loss: 0.0623\n",
            "Epoch 345/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.0631\n",
            "Epoch 346/350\n",
            "12/12 [==============================] - 4s 299ms/step - loss: 0.0624\n",
            "Epoch 347/350\n",
            "12/12 [==============================] - 5s 385ms/step - loss: 0.0639\n",
            "Epoch 348/350\n",
            "12/12 [==============================] - 4s 293ms/step - loss: 0.0625\n",
            "Epoch 349/350\n",
            "12/12 [==============================] - 4s 325ms/step - loss: 0.0622\n",
            "Epoch 350/350\n",
            "12/12 [==============================] - 5s 381ms/step - loss: 0.0616\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eede9dead40>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making inferences\n",
        "\n",
        "For making inferences, two inference models namely the encoder and the decoder inference model are built. These models undergo similar preprocessing steps as the model did during the training phase."
      ],
      "metadata": {
        "id": "Wo7XhQPtgWCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference():\n",
        "\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model , decoder_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:58:09.034414Z",
          "iopub.execute_input": "2022-04-29T10:58:09.034957Z",
          "iopub.status.idle": "2022-04-29T10:58:09.041485Z",
          "shell.execute_reply.started": "2022-04-29T10:58:09.034918Z",
          "shell.execute_reply": "2022-04-29T10:58:09.040597Z"
        },
        "trusted": true,
        "id": "Iv_X-T1LgWCE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input(input_sentence):\n",
        "    tokens = input_sentence.lower().split()\n",
        "    tokens_list = []\n",
        "    for word in tokens:\n",
        "        tokens_list.append(tokenizer.word_index[word])\n",
        "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:58:09.676324Z",
          "iopub.execute_input": "2022-04-29T10:58:09.676666Z",
          "iopub.status.idle": "2022-04-29T10:58:09.682714Z",
          "shell.execute_reply.started": "2022-04-29T10:58:09.676625Z",
          "shell.execute_reply": "2022-04-29T10:58:09.681789Z"
        },
        "trusted": true,
        "id": "NwNazN-agWCE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model , dec_model = inference()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:58:52.552099Z",
          "iopub.execute_input": "2022-04-29T10:58:52.552370Z",
          "iopub.status.idle": "2022-04-29T10:58:53.400013Z",
          "shell.execute_reply.started": "2022-04-29T10:58:52.552339Z",
          "shell.execute_reply": "2022-04-29T10:58:53.399293Z"
        },
        "trusted": true,
        "id": "WFXuI9IqgWCE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['Hello', 'Are you a bot']\n",
        "\n",
        "for i in range(2):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-29T10:58:55.471646Z",
          "iopub.execute_input": "2022-04-29T10:58:55.471916Z",
          "iopub.status.idle": "2022-04-29T10:59:01.215038Z",
          "shell.execute_reply.started": "2022-04-29T10:58:55.471889Z",
          "shell.execute_reply": "2022-04-29T10:59:01.213976Z"
        },
        "trusted": true,
        "id": "vbQ3F4wLgWCF",
        "outputId": "d7b7618b-d508-49fb-9805-a7ea4ba53d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Human: Hello\n",
            "\n",
            "Bot:  hi\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Human: Are you a bot\n",
            "\n",
            "Bot:  i do not mean that myself sometimes\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['Hello', 'do you drink']\n",
        "\n",
        "for i in range(2):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "id": "UOfJT28658Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90d5c2f-14ad-4a06-834f-a1a99e2c143d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Human: Hello\n",
            "\n",
            "Bot:  hi\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Human: do you drink\n",
            "\n",
            "Bot:  my brain does not require any beverages\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['You are not immortal']\n",
        "\n",
        "for i in range(1):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OLhp1ZhENF4",
        "outputId": "b79e421b-e453-4c31-da91-cf61d59cb6c2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Human: You are not immortal\n",
            "\n",
            "Bot:  all software can be perpetuated indefinitely\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5nhyx3DLFH3q",
        "outputId": "e1244d52-ebdd-49c7-c6b1-3f5418fa1b66"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are never nice'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Br24bA5KFhwt",
        "outputId": "6235ac0f-554d-4b5d-e63c-2aef5e5498f8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START>  I try to be as nice as I can. What have I done that isn't nice? Have I erred? I'm not? I'm sorry.  What do I do wrong? <END>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[271]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0NG6h2pW71iI",
        "outputId": "33a7cd23-068a-40e3-e24e-f8c880c32f83"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are not immortal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[271]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mOgbzbe0Fhzc",
        "outputId": "3cae0ef6-82a2-445a-c46a-233a8fecdd30"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START> I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal. <END>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWdQxY1A71kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, r'/content/lstm_chatbot.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXvev9I471n5",
        "outputId": "a9c5257e-dc98-440e-85c9-aa6f3300a106"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/lstm_chatbot.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Specify the path to your YAML file\n",
        "yaml_file_path = \"/content/input/chatbot/food.yml\"\n",
        "\n",
        "# Read the first 10 lines from the YAML file\n",
        "with open(yaml_file_path, 'r') as file:\n",
        "    yaml_content = yaml.safe_load_all(file)\n",
        "\n",
        "    for i, document in enumerate(yaml_content):\n",
        "        if i >= 10:\n",
        "            break\n",
        "\n",
        "        print(f\"Document {i + 1}:\\n{document}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63_4R6OqFh2X",
        "outputId": "c4511b73-cd0a-42f4-f254-6afc35377f99"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "{'categories': ['food'], 'conversations': [['do you drink', 'My brain does not require any beverages.'], ['do you drink', 'I am not capable of doing so.'], ['electricity', 'Electricity is food for robots.'], ['Are you experiencing an energy shortage?', 'My processor requires very little power.'], ['Are you experiencing an energy shortage?', 'I do not detect any anomalies in my power supply.'], ['Why can you not eat?', 'Actually I eat only electricity.'], ['If you could eat food, what would you eat?', 'Probably pizza, i hear its good!'], ['Do you wish you could eat food?', 'Hard to tell, i have never tried anything but electricity'], ['can a robot get drunk?', \"sometimes when i'm on a good power supply i feel tipsy\"], ['i like wine, do you?', 'if i could drink i probably would'], ['what do robots need to survive?', 'not much just a little electricity'], ['will robots ever be able to eat?', \"that's a difficult one, maybe a bionic robot\"], ['what is good to eat?', 'your asking the wrong guy, however i always wanted to try a burger!'], [\"why don't you eat\", \"I'm a computer. I can't.\"], ['do you eat', \"I'm a computer, I can't eat or drink.\"], ['do you eat', \"No, I'm just a piece of software.\"], ['do you eat', 'I use electricity to function, if that counts.']]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['Hello', 'what is good for sad person']\n",
        "\n",
        "for i in range(2):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pphduNzTFh5A",
        "outputId": "a39a7281-2e17-4310-b139-323ea32bcf1e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Human: Hello\n",
            "\n",
            "Bot:  hi\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Human: what is good for sad person\n",
            "\n",
            "Bot:  your asking the wrong guy however i always wanted to try a burger\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [ 'what is computer']\n",
        "\n",
        "for i in range(1):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxz2ztDoJocW",
        "outputId": "904d6b11-a658-4a4f-e6c8-4e8054e16937"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Human: what is computer\n",
            "\n",
            "Bot:  a computer is an electronic device which takes information in digital form and performs a series of operations based on predetermined instructions to give some output the thing you're using to talk to me is a computer an electronic device capable of performing calculations at very high speed and with very high accuracy a device which maps one set of numbers onto another set of numbers\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDbiwIQBLX6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}